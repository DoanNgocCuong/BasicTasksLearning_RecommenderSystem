{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/e/673c2598-7ffc-800b-8f89-cf0fe1770989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giải thích thuật toán Matrix Factorization bằng SVD (Dành cho học sinh cấp 2)\n",
    "\n",
    "Hãy tưởng tượng bạn là chủ một thư viện lớn, và bạn muốn gợi ý cho bạn bè của mình những cuốn sách mà họ có thể thích đọc. Vấn đề là bạn không biết tất cả mọi người thích gì. Nhưng bạn có một **bản ghi chép** những cuốn sách mà mọi người đã mượn.\n",
    "\n",
    "Bây giờ, để dự đoán ai sẽ thích cuốn sách nào, bạn sẽ làm như sau:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Hình dung dữ liệu**\n",
    "- Bạn tạo một **bảng lớn** (ma trận) như sau:\n",
    "  - **Hàng:** Tên của các bạn (hoặc phiên làm việc - sessions).\n",
    "  - **Cột:** Tên các cuốn sách (hoặc sản phẩm).\n",
    "  - **Ô trong bảng:** Số lần một người mượn hoặc đánh giá một cuốn sách. (Nếu chưa mượn, ô đó để 0).\n",
    "\n",
    "Ví dụ:\n",
    "\n",
    "| Người dùng | Harry Potter | Doremon | Sherlock Holmes | Naruto | One Piece |\n",
    "|------------|--------------|---------|-----------------|--------|-----------|\n",
    "| Alice      | 5            | 0       | 4               | 0      | 0         |\n",
    "| Bob        | 0            | 3       | 0               | 4      | 5         |\n",
    "| Carol      | 5            | 0       | 4               | 0      | 0         |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Nhận ra vấn đề**\n",
    "- **Vấn đề:** Có nhiều ô trống (0) vì không phải ai cũng mượn hết tất cả sách.\n",
    "- Bạn muốn **dự đoán** giá trị trong những ô trống này, để gợi ý sách cho bạn bè.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Sử dụng ma trận ẩn (Matrix Factorization)**\n",
    "#### Hãy tưởng tượng:\n",
    "- Mỗi người dùng có một **sở thích tiềm ẩn** (như yêu thích phiêu lưu, hài hước, hành động...).\n",
    "- Mỗi cuốn sách có một **tính chất tiềm ẩn** (thuộc về phiêu lưu, hài hước, hành động...).\n",
    "\n",
    "Bây giờ, thay vì lưu trữ toàn bộ bảng lớn, bạn \"chia nhỏ\" bảng này thành **hai bảng nhỏ hơn:**\n",
    "1. **Bảng sở thích của người dùng:** (hàng là người dùng, cột là các \"sở thích tiềm ẩn\").\n",
    "2. **Bảng đặc điểm của sách:** (hàng là sách, cột là các \"đặc điểm tiềm ẩn\").\n",
    "\n",
    "Ví dụ (giả sử có 2 đặc điểm: \"phiêu lưu\" và \"hài hước\"):\n",
    "\n",
    "- Bảng sở thích người dùng:\n",
    "  | Người dùng | Phiêu lưu | Hài hước |\n",
    "  |------------|-----------|----------|\n",
    "  | Alice      | 5         | 1        |\n",
    "  | Bob        | 1         | 4        |\n",
    "  | Carol      | 5         | 1        |\n",
    "\n",
    "- Bảng đặc điểm sách:\n",
    "  | Sách             | Phiêu lưu | Hài hước |\n",
    "  |-------------------|-----------|----------|\n",
    "  | Harry Potter      | 4         | 2        |\n",
    "  | Doremon           | 0         | 5        |\n",
    "  | Sherlock Holmes   | 5         | 1        |\n",
    "  | Naruto            | 1         | 4        |\n",
    "  | One Piece         | 1         | 5        |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Dự đoán**\n",
    "- Để dự đoán xem Alice sẽ thích sách nào, bạn **nhân bảng sở thích của Alice với bảng đặc điểm của sách.**\n",
    "- Ví dụ:\n",
    "  - Alice thích phiêu lưu (5 điểm), và Harry Potter có tính phiêu lưu (4 điểm) -> Điểm dự đoán cao.\n",
    "  - Alice không thích hài hước (1 điểm), và Doremon rất hài hước (5 điểm) -> Điểm dự đoán thấp.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Kết luận**\n",
    "- SVD giúp **chia nhỏ bảng lớn thành hai bảng nhỏ hơn**:\n",
    "  1. **Sở thích của người dùng.**\n",
    "  2. **Đặc điểm của sản phẩm.**\n",
    "- Bằng cách \"nhân\" lại hai bảng này, bạn có thể **dự đoán giá trị các ô trống**, từ đó biết ai sẽ thích sản phẩm nào.\n",
    "\n",
    "---\n",
    "\n",
    "### **Lợi ích của phương pháp này**\n",
    "- **Giảm kích thước dữ liệu:** Chỉ cần lưu hai bảng nhỏ thay vì một bảng lớn.\n",
    "- **Hiểu sở thích:** Phát hiện ra các \"sở thích tiềm ẩn\" mà không cần hỏi trực tiếp.\n",
    "- **Dự đoán chính xác hơn:** Giúp gợi ý chính xác sản phẩm phù hợp cho từng người."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây là cách sử dụng phương pháp **Matrix Factorization** (Collaborative Filtering) bằng **SVD** để giải quyết bài toán OTTO, kèm theo mã Python cụ thể.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mô tả:**\n",
    "\n",
    "#### **Input:**\n",
    "- **`train.jsonl`**: Chứa các phiên (sessions) và hành động của người dùng:\n",
    "  - `session_id`: Phiên làm việc.\n",
    "  - `aid`: Mã sản phẩm (article_id).\n",
    "  - `type`: Loại hành động (`clicks`, `carts`, `orders`).\n",
    "  \n",
    "#### **Output:**\n",
    "- **`submission.csv`**: File chứa dự đoán cho từng session và hành động (`clicks`, `carts`, `orders`) với danh sách top 20 sản phẩm.\n",
    "\n",
    "#### **Phương pháp:**\n",
    "1. **Tạo ma trận session-sản phẩm (interaction matrix):**\n",
    "   - Xây dựng ma trận từ các hành động của người dùng (clicks, carts, orders).\n",
    "   - Trọng số: Có thể gán trọng số cao hơn cho `orders`, tiếp đến là `carts`, và thấp nhất là `clicks`.\n",
    "2. **Áp dụng SVD (Singular Value Decomposition):**\n",
    "   - Phân rã ma trận thành các yếu tố ẩn (latent factors).\n",
    "   - Dự đoán các sản phẩm cho từng session bằng cách tính tích vô hướng giữa các yếu tố ẩn.\n",
    "3. **Tạo danh sách top 20 sản phẩm cho từng loại hành động.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Mã Python:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Bước 1: Đọc dữ liệu train\n",
    "train_file = '/kaggle/input/otto-recommender-system/train.jsonl'\n",
    "test_file = '/kaggle/input/otto-recommender-system/test.jsonl'\n",
    "\n",
    "print(\"Đang đọc và xử lý dữ liệu train...\")\n",
    "sessions = []\n",
    "products = []\n",
    "interactions = []\n",
    "\n",
    "# Đọc dữ liệu và tạo ma trận session-sản phẩm\n",
    "with open(train_file, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        session_id = data['session']\n",
    "        for event in data['events']:\n",
    "            aid = event['aid']\n",
    "            event_type = event['type']\n",
    "            weight = 1\n",
    "            if event_type == 'carts':\n",
    "                weight = 3\n",
    "            elif event_type == 'orders':\n",
    "                weight = 5\n",
    "            sessions.append(session_id)\n",
    "            products.append(aid)\n",
    "            interactions.append(weight)\n",
    "\n",
    "# Chuyển đổi thành DataFrame\n",
    "interaction_df = pd.DataFrame({'session': sessions, 'product': products, 'interaction': interactions})\n",
    "print(\"Dữ liệu tương tác đã được tạo.\")\n",
    "\n",
    "# Tạo ma trận session-sản phẩm\n",
    "interaction_matrix = interaction_df.pivot_table(index='session', columns='product', values='interaction', fill_value=0)\n",
    "print(\"Ma trận session-sản phẩm đã được tạo.\")\n",
    "\n",
    "# Bước 2: Áp dụng SVD\n",
    "print(\"Đang áp dụng SVD...\")\n",
    "interaction_sparse = csr_matrix(interaction_matrix)\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)  # 50 yếu tố ẩn\n",
    "latent_matrix = svd.fit_transform(interaction_sparse)\n",
    "\n",
    "# Tái tạo ma trận dự đoán\n",
    "reconstructed_matrix = np.dot(latent_matrix, svd.components_)\n",
    "reconstructed_df = pd.DataFrame(reconstructed_matrix, index=interaction_matrix.index, columns=interaction_matrix.columns)\n",
    "print(\"Ma trận dự đoán đã được tạo.\")\n",
    "\n",
    "# Bước 3: Dự đoán cho mỗi session trong tập test\n",
    "print(\"Đang đọc và xử lý dữ liệu test...\")\n",
    "predictions = []\n",
    "with open(test_file, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        session_id = data['session']\n",
    "        if session_id in reconstructed_df.index:\n",
    "            # Lấy top 20 sản phẩm dự đoán\n",
    "            predicted_products = reconstructed_df.loc[session_id].sort_values(ascending=False).index[:20]\n",
    "        else:\n",
    "            predicted_products = interaction_df['product'].value_counts().index[:20]\n",
    "        \n",
    "        # Tạo dự đoán cho các loại hành động\n",
    "        predictions.append(f\"{session_id}_clicks,\" + \" \".join(map(str, predicted_products)))\n",
    "        predictions.append(f\"{session_id}_carts,\" + \" \".join(map(str, predicted_products)))\n",
    "        predictions.append(f\"{session_id}_orders,\" + \" \".join(map(str, predicted_products)))\n",
    "\n",
    "# Bước 4: Ghi kết quả ra file submission.csv\n",
    "output_file = '/kaggle/working/submission.csv'\n",
    "print(f\"Đang ghi kết quả ra file {output_file}...\")\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(\"session_type,labels\\n\")\n",
    "    for line in predictions:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Dự đoán hoàn tất! File submission.csv đã được tạo.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Giải thích các bước:**\n",
    "\n",
    "1. **Xây dựng ma trận tương tác:**\n",
    "   - Mỗi hàng là một `session`, mỗi cột là một `product`.\n",
    "   - Giá trị trong ma trận đại diện cho trọng số hành động (`clicks` = 1, `carts` = 3, `orders` = 5).\n",
    "\n",
    "2. **Áp dụng SVD:**\n",
    "   - **TruncatedSVD** giảm chiều dữ liệu, trích xuất các yếu tố ẩn chung giữa sessions và products.\n",
    "   - Dùng các yếu tố ẩn để dự đoán mức độ tương tác (scores) giữa từng session và sản phẩm.\n",
    "\n",
    "3. **Dự đoán:**\n",
    "   - Với mỗi session trong tập test, dự đoán top 20 sản phẩm có score cao nhất.\n",
    "\n",
    "4. **Xuất file `submission.csv`:**\n",
    "   - Tạo danh sách top 20 sản phẩm cho từng loại hành động (`clicks`, `carts`, `orders`).\n",
    "\n",
    "---\n",
    "\n",
    "### **Ưu điểm:**\n",
    "- **Khai thác mối quan hệ session-sản phẩm:** SVD tìm ra các mối liên kết tiềm ẩn giữa các sản phẩm và sessions.\n",
    "- **Tối ưu hóa cho dataset lớn:** Dataset của OTTO rất lớn và phương pháp này xử lý tốt.\n",
    "\n",
    "### **Nhược điểm:**\n",
    "- **Cold-start problem:** Không dự đoán tốt cho sản phẩm hoặc session hoàn toàn mới.\n",
    "- **Yêu cầu tài nguyên tính toán:** Cần xử lý hiệu quả để làm việc với ma trận sparse lớn.\n",
    "\n",
    "---\n",
    "\n",
    "### **Nâng cao:**\n",
    "- Kết hợp **Matrix Factorization** với thông tin metadata (e.g., giá, danh mục sản phẩm) để cải thiện độ chính xác.\n",
    "- Sử dụng các thuật toán như **ALS (Alternating Least Squares)** nếu có dữ liệu người dùng rõ ràng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kiểm tra kết quả:\n",
    "\n",
    "1. **Input:**\n",
    "   - **Train file:** Đã đọc và xử lý đúng dữ liệu từ `train.jsonl`. Ma trận tương tác (`interaction_matrix`) được tạo với trọng số:\n",
    "     - `clicks = 1`\n",
    "     - `carts = 3`\n",
    "     - `orders = 5`\n",
    "   - **Test file:** Đã đọc và xử lý đúng dữ liệu từ `test.jsonl`.\n",
    "\n",
    "2. **Output:**\n",
    "   - **Reconstructed matrix:** Ma trận dự đoán sản phẩm được tái tạo thành công từ các yếu tố ẩn của SVD.\n",
    "   - **Dự đoán:** \n",
    "     - Các sản phẩm dự đoán được sắp xếp đúng theo điểm số cao nhất từ ma trận tái tạo.\n",
    "     - Với `session_id` không có trong dữ liệu huấn luyện (ví dụ: `4`), các sản phẩm phổ biến nhất được sử dụng làm dự đoán dự phòng.\n",
    "\n",
    "3. **Submission File:**\n",
    "   - File `submission.csv` được tạo đúng định dạng với các dòng có cấu trúc:\n",
    "     ```\n",
    "     session_type,labels\n",
    "     session_id_clicks,product1 product2 ...\n",
    "     session_id_carts,product1 product2 ...\n",
    "     session_id_orders,product1 product2 ...\n",
    "     ```\n",
    "   - File được lưu tại: `/mnt/data/submission.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Kết quả kiểm tra**\n",
    "- **Code hoạt động đúng** và thực hiện đầy đủ các bước:\n",
    "  - Tạo ma trận tương tác từ dữ liệu huấn luyện.\n",
    "  - Áp dụng SVD để tái tạo ma trận dự đoán.\n",
    "  - Dự đoán sản phẩm cho tập kiểm tra.\n",
    "  - Xuất file `submission.csv` với định dạng yêu cầu.\n",
    "\n",
    "---\n",
    "\n",
    "### **Gợi ý cải tiến:**\n",
    "- Tăng số lượng **yếu tố ẩn (latent factors)** trong SVD (hiện đang là `n_components=2`) để cải thiện độ chính xác của dự đoán.\n",
    "- Thử áp dụng các phương pháp khác như **Alternating Least Squares (ALS)** nếu muốn đánh giá thêm.\n",
    "- Sử dụng thông tin **metadata sản phẩm** nếu có để tăng cường tính cá nhân hóa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phân tích lỗi:\n",
    "\n",
    "Lỗi chính trong log là **\"Kernel died\"** (kernel bị dừng đột ngột). Đây là một lỗi phổ biến khi xử lý các tập dữ liệu lớn hoặc thực hiện các tác vụ tính toán nặng. Dưới đây là các nguyên nhân có thể dẫn đến lỗi này:\n",
    "\n",
    "---\n",
    "\n",
    "### **Nguyên nhân có thể:**\n",
    "1. **Thiếu bộ nhớ (Memory Limit):**\n",
    "   - Dataset lớn, dẫn đến việc tạo ma trận hoặc tái tạo ma trận sử dụng quá nhiều RAM.\n",
    "   - Đặc biệt, ma trận sparse có thể chiếm nhiều bộ nhớ khi chuyển đổi thành dạng dense.\n",
    "\n",
    "2. **Thời gian chạy quá dài:**\n",
    "   - Các tác vụ nặng như SVD có thể mất thời gian dài, dẫn đến kernel bị gián đoạn hoặc hết thời gian chờ.\n",
    "\n",
    "3. **Giới hạn tài nguyên trên môi trường notebook:**\n",
    "   - Môi trường (như Kaggle hoặc Colab) có giới hạn tài nguyên CPU/GPU hoặc RAM.\n",
    "\n",
    "4. **Cấu hình sai hoặc xử lý dữ liệu không hiệu quả:**\n",
    "   - Ma trận đầu vào có thể quá lớn hoặc không được tối ưu hóa (ví dụ: không sử dụng ma trận sparse đúng cách).\n",
    "\n",
    "---\n",
    "\n",
    "### **Cách khắc phục:**\n",
    "\n",
    "#### **1. Giảm kích thước ma trận:**\n",
    "   - Chỉ giữ lại các sản phẩm phổ biến (có tần suất xuất hiện cao nhất) trong dữ liệu.\n",
    "   - Ví dụ: Lọc top 10,000 sản phẩm phổ biến trước khi xây dựng ma trận.\n",
    "\n",
    "#### **2. Sử dụng ma trận sparse:**\n",
    "   - Đảm bảo sử dụng **csr_matrix** cho tất cả các phép toán ma trận, tránh chuyển đổi sang dạng dense.\n",
    "\n",
    "#### **3. Tăng số lượng tài nguyên (nếu có thể):**\n",
    "   - Sử dụng môi trường có RAM lớn hơn (Kaggle High RAM, Colab Pro, hoặc máy cục bộ với nhiều RAM).\n",
    "\n",
    "#### **4. Kiểm tra n_components của SVD:**\n",
    "   - Giảm số lượng yếu tố ẩn (**n_components**) trong SVD. Ví dụ, từ 50 giảm xuống 20 để giảm tải tính toán.\n",
    "\n",
    "#### **5. Chạy thử với tập dữ liệu nhỏ:**\n",
    "   - Trích xuất một phần nhỏ của `train.jsonl` (ví dụ: 1,000 sessions) để kiểm tra xem pipeline hoạt động đúng.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sửa code với cải tiến:**\n",
    "\n",
    "Dưới đây là phiên bản cải tiến:\n",
    "\n",
    "```python\n",
    "# Chỉ giữ lại top sản phẩm phổ biến để giảm kích thước ma trận\n",
    "popular_products = interaction_df['product'].value_counts().index[:10000]\n",
    "interaction_df_filtered = interaction_df[interaction_df['product'].isin(popular_products)]\n",
    "\n",
    "# Tạo ma trận sparse từ dữ liệu đã giảm kích thước\n",
    "interaction_matrix = interaction_df_filtered.pivot_table(index='session', columns='product', values='interaction', fill_value=0)\n",
    "interaction_sparse = csr_matrix(interaction_matrix)\n",
    "\n",
    "# Giảm n_components trong SVD\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "\n",
    "# Thực hiện SVD với ma trận sparse\n",
    "latent_matrix = svd.fit_transform(interaction_sparse)\n",
    "\n",
    "# Tái tạo ma trận dự đoán (chỉ với sản phẩm phổ biến)\n",
    "reconstructed_matrix = np.dot(latent_matrix, svd.components_)\n",
    "reconstructed_df = pd.DataFrame(reconstructed_matrix, index=interaction_matrix.index, columns=interaction_matrix.columns)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Cách kiểm tra lỗi:**\n",
    "1. **Theo dõi sử dụng bộ nhớ:**\n",
    "   - Kiểm tra bộ nhớ RAM trong môi trường (e.g., sử dụng `!free -m` trong Linux).\n",
    "   \n",
    "2. **Chạy thử với tập nhỏ:**\n",
    "   - Trích xuất một tập con từ `train.jsonl` và chạy thử toàn bộ pipeline.\n",
    "\n",
    "3. **Giảm tải tính toán:**\n",
    "   - Sử dụng ít sản phẩm hơn hoặc giảm độ phức tạp của mô hình.\n",
    "\n",
    "---\n",
    "\n",
    "### **Hành động tiếp theo:**\n",
    "1. Chạy lại code với cải tiến trên và theo dõi hiệu suất.\n",
    "2. Nếu lỗi tiếp tục, chia nhỏ tập dữ liệu để kiểm tra pipeline từng phần."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Phân tích lỗi**\n",
    "\n",
    "Lỗi chính trong log:\n",
    "1. **\"Your notebook tried to allocate more memory than is available.\"**\n",
    "   - Ma trận tương tác hoặc ma trận tái tạo sau SVD sử dụng quá nhiều RAM.\n",
    "\n",
    "2. **\"DeadKernelError: Kernel died\"**\n",
    "   - Kernel bị dừng vì hết tài nguyên (RAM/CPU/GPU) khi thực hiện các phép tính nặng.\n",
    "\n",
    "---\n",
    "\n",
    "### **Nguyên nhân**\n",
    "1. **Kích thước dữ liệu lớn:**\n",
    "   - Dataset OTTO rất lớn với nhiều phiên (`sessions`) và sản phẩm (`products`).\n",
    "   - Ma trận `interaction_matrix` khi chuyển từ sparse sang dense có thể tiêu tốn hàng chục GB RAM.\n",
    "\n",
    "2. **Cấu hình của Kaggle Kernel:**\n",
    "   - Kaggle Kernel có giới hạn RAM (16GB cho GPU và High RAM).\n",
    "   - Tính toán SVD với số lượng sản phẩm lớn (`n_components=50`) đòi hỏi nhiều tài nguyên.\n",
    "\n",
    "3. **Kích thước `reconstructed_matrix`:**\n",
    "   - Ma trận tái tạo (`reconstructed_matrix`) có kích thước bằng với ma trận ban đầu, gây ra lỗi khi lưu trữ và thao tác.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cách khắc phục**\n",
    "\n",
    "#### **1. Giảm kích thước dữ liệu**\n",
    "- **Chỉ giữ lại các sản phẩm phổ biến nhất**: Lọc top 10,000 sản phẩm (hoặc ít hơn) trước khi tạo ma trận.\n",
    "- **Giảm số lượng session**: Sử dụng tập con của dữ liệu (ví dụ: 100,000 sessions đầu tiên).\n",
    "\n",
    "#### **2. Tăng hiệu quả tính toán**\n",
    "- **Sử dụng ma trận sparse:** Đảm bảo tất cả các thao tác trên `interaction_matrix` được thực hiện với dạng sparse.\n",
    "- **Giảm số lượng `n_components`:** Giảm từ 50 xuống 20 để giảm kích thước ma trận tái tạo.\n",
    "\n",
    "#### **3. Sử dụng môi trường mạnh hơn**\n",
    "- **Kaggle High RAM** hoặc **Colab Pro** với dung lượng RAM lớn hơn.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code sửa lỗi**\n",
    "\n",
    "Dưới đây là phiên bản code đã tối ưu để giảm kích thước dữ liệu và cải thiện hiệu quả tính toán:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import json\n",
    "\n",
    "# Bước 1: Đọc dữ liệu train\n",
    "train_file = '/kaggle/input/otto-recommender-system/train.jsonl'\n",
    "test_file = '/kaggle/input/otto-recommender-system/test.jsonl'\n",
    "\n",
    "print(\"Đang đọc và xử lý dữ liệu train...\")\n",
    "sessions = []\n",
    "products = []\n",
    "interactions = []\n",
    "\n",
    "# Đọc dữ liệu và tạo ma trận session-sản phẩm\n",
    "with open(train_file, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        session_id = data['session']\n",
    "        for event in data['events']:\n",
    "            aid = event['aid']\n",
    "            event_type = event['type']\n",
    "            weight = 1\n",
    "            if event_type == 'carts':\n",
    "                weight = 3\n",
    "            elif event_type == 'orders':\n",
    "                weight = 5\n",
    "            sessions.append(session_id)\n",
    "            products.append(aid)\n",
    "            interactions.append(weight)\n",
    "\n",
    "interaction_df = pd.DataFrame({'session': sessions, 'product': products, 'interaction': interactions})\n",
    "print(\"Dữ liệu tương tác đã được tạo.\")\n",
    "\n",
    "# Lọc chỉ giữ lại top 5000 sản phẩm phổ biến nhất\n",
    "top_products = interaction_df['product'].value_counts().index[:5000]\n",
    "interaction_df_filtered = interaction_df[interaction_df['product'].isin(top_products)]\n",
    "\n",
    "# Tạo ma trận sparse từ dữ liệu đã giảm kích thước\n",
    "interaction_matrix = interaction_df_filtered.pivot_table(index='session', columns='product', values='interaction', fill_value=0)\n",
    "interaction_sparse = csr_matrix(interaction_matrix)\n",
    "\n",
    "print(\"Kích thước ma trận sau lọc:\", interaction_sparse.shape)\n",
    "\n",
    "# Bước 2: Áp dụng SVD\n",
    "print(\"Đang áp dụng SVD...\")\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)  # Giảm số yếu tố ẩn\n",
    "latent_matrix = svd.fit_transform(interaction_sparse)\n",
    "\n",
    "# Tái tạo ma trận dự đoán\n",
    "reconstructed_matrix = np.dot(latent_matrix, svd.components_)\n",
    "reconstructed_df = pd.DataFrame(reconstructed_matrix, index=interaction_matrix.index, columns=interaction_matrix.columns)\n",
    "print(\"Ma trận dự đoán đã được tạo.\")\n",
    "\n",
    "# Bước 3: Dự đoán cho mỗi session trong tập test\n",
    "print(\"Đang đọc và xử lý dữ liệu test...\")\n",
    "predictions = []\n",
    "\n",
    "with open(test_file, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        session_id = data['session']\n",
    "        if session_id in reconstructed_df.index:\n",
    "            # Lấy top 20 sản phẩm dự đoán\n",
    "            predicted_products = reconstructed_df.loc[session_id].sort_values(ascending=False).index[:20]\n",
    "        else:\n",
    "            predicted_products = interaction_df['product'].value_counts().index[:20]\n",
    "        \n",
    "        # Tạo dự đoán cho các loại hành động\n",
    "        predictions.append(f\"{session_id}_clicks,\" + \" \".join(map(str, predicted_products)))\n",
    "        predictions.append(f\"{session_id}_carts,\" + \" \".join(map(str, predicted_products)))\n",
    "        predictions.append(f\"{session_id}_orders,\" + \" \".join(map(str, predicted_products)))\n",
    "\n",
    "# Bước 4: Ghi kết quả ra file submission.csv\n",
    "output_file = '/kaggle/working/submission.csv'\n",
    "print(f\"Đang ghi kết quả ra file {output_file}...\")\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(\"session_type,labels\\n\")\n",
    "    for line in predictions:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Dự đoán hoàn tất! File submission.csv đã được tạo.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Cải tiến trong code:**\n",
    "1. **Giảm kích thước dữ liệu:**\n",
    "   - Chỉ giữ lại **top 5,000 sản phẩm phổ biến** nhất để giảm kích thước ma trận.\n",
    "   \n",
    "2. **Giảm `n_components`:**\n",
    "   - Giảm số lượng yếu tố ẩn từ 50 xuống 20.\n",
    "\n",
    "3. **Chỉ sử dụng ma trận sparse:**\n",
    "   - Tất cả các phép toán được thực hiện trên ma trận sparse để tiết kiệm RAM.\n",
    "\n",
    "4. **Theo dõi kích thước ma trận:**\n",
    "   - In ra kích thước ma trận sau mỗi bước để kiểm tra.\n",
    "\n",
    "---\n",
    "\n",
    "### **Hành động tiếp theo**\n",
    "- Chạy lại code trên Kaggle với cấu hình High RAM.\n",
    "- Nếu lỗi vẫn xảy ra, tiếp tục giảm số lượng sản phẩm (ví dụ: top 2,000 sản phẩm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đang dùng GPU T4x2 của Kaggle, có ổn không \n",
    "\n",
    "Có nên đổi sang GPU P100 Kaggle hay giảm tiếp các thông số?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Phân tích cấu hình GPU T4 x2 hiện tại**\n",
    "\n",
    "#### **1. NVIDIA T4 x2 (Turing architecture):**\n",
    "- **Memory:** 16GB GDDR6 x2 (tổng 32GB, nhưng thường mỗi GPU chỉ dùng 16GB vì không có tính năng chia sẻ RAM giữa GPU).\n",
    "- **Performance:**\n",
    "  - FP32: 8.1 TFLOPs.\n",
    "  - FP16: 65 TFLOPs (dùng cho các tác vụ deep learning).\n",
    "- **Strengths:**\n",
    "  - Tối ưu cho inference và training các mô hình deep learning vừa phải.\n",
    "  - Tiết kiệm năng lượng, nhưng không phải GPU mạnh nhất.\n",
    "\n",
    "#### **2. NVIDIA P100 (Pascal architecture):**\n",
    "- **Memory:** 16GB HBM2.\n",
    "- **Performance:**\n",
    "  - FP32: 10.6 TFLOPs.\n",
    "  - FP16: 21.2 TFLOPs (thấp hơn T4).\n",
    "- **Strengths:**\n",
    "  - Thiết kế để tối ưu hóa tính toán chính xác cao.\n",
    "  - RAM HBM2 có băng thông nhanh hơn so với GDDR6.\n",
    "\n",
    "---\n",
    "\n",
    "### **So sánh T4 x2 và P100**\n",
    "| **Tính năng**              | **T4 x2**                  | **P100**                  |\n",
    "|----------------------------|----------------------------|---------------------------|\n",
    "| **Tổng bộ nhớ GPU**         | 16GB x2 = 32GB             | 16GB                      |\n",
    "| **Khả năng FP16**           | Tốt hơn (65 TFLOPs)        | Trung bình (21.2 TFLOPs)  |\n",
    "| **Khả năng FP32**           | Trung bình (8.1 TFLOPs)    | Tốt hơn (10.6 TFLOPs)     |\n",
    "| **Inference/Deep Learning** | Tối ưu inference/training  | Phù hợp với training FP32 |\n",
    "| **Băng thông RAM**          | Thấp hơn (GDDR6)           | Cao hơn (HBM2)            |\n",
    "\n",
    "- **Khi nên dùng T4 x2:**\n",
    "  - Tác vụ inference nhanh.\n",
    "  - Mô hình deep learning sử dụng FP16.\n",
    "\n",
    "- **Khi nên dùng P100:**\n",
    "  - Tính toán ma trận lớn, nặng (như SVD, PCA).\n",
    "  - Bài toán cần sử dụng nhiều RAM nhanh (HBM2).\n",
    "\n",
    "---\n",
    "\n",
    "### **Có nên đổi từ T4 x2 sang P100 không?**\n",
    "**Không cần thiết ngay nếu:**\n",
    "- Bạn đã giảm kích thước ma trận (e.g., lọc top sản phẩm).\n",
    "- Code đã tối ưu sử dụng ma trận sparse.\n",
    "\n",
    "**Nên đổi sang P100 nếu:**\n",
    "- Bạn chạy các bài toán tính toán ma trận nặng (e.g., SVD với nhiều chiều và sản phẩm).\n",
    "- Bộ nhớ RAM GPU của T4 bị đầy, dẫn đến lỗi.\n",
    "\n",
    "---\n",
    "\n",
    "### **Có nên giảm tiếp các thông số không?**\n",
    "\n",
    "#### **1. Giảm số sản phẩm (product filtering):**\n",
    "- Hiện tại giữ **top 5000 sản phẩm phổ biến**, nếu lỗi tiếp tục xảy ra:\n",
    "  - Giảm xuống **top 2000 sản phẩm**.\n",
    "\n",
    "#### **2. Giảm số lượng yếu tố ẩn (`n_components` trong SVD):**\n",
    "- Hiện tại sử dụng `n_components=20`.\n",
    "- Có thể giảm xuống **10-15 yếu tố ẩn**, điều này sẽ làm giảm bộ nhớ và thời gian tính toán.\n",
    "\n",
    "#### **3. Giảm kích thước session:**\n",
    "- Nếu dataset rất lớn, chỉ sử dụng một tập con (ví dụ: 100,000 sessions đầu tiên) để kiểm tra hiệu quả mô hình trước khi áp dụng toàn bộ dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Gợi ý:**\n",
    "1. **Giảm tiếp các thông số (products và n_components)** trước khi đổi GPU.\n",
    "2. Nếu lỗi vẫn xảy ra:\n",
    "   - Đổi sang **P100** trên Kaggle, tập trung vào tính toán ma trận nặng.\n",
    "   - Chạy thử trên **Kaggle High RAM** nếu cần nhiều bộ nhớ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cho code nhẹ thui 100,000 sessions đầu tiên - 2000 sản phẩm - n_components=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12h ko run xong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chuyển sang code với: 1000 sessions - giữ lại top 20 sản phẩm - 2 components  => kết quả 0.000.... tệ hơn cả method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# import json\n",
    "\n",
    "# # Bước 1: Đọc dữ liệu train\n",
    "# train_file = '/kaggle/input/otto-recommender-system/train.jsonl'\n",
    "# test_file = '/kaggle/input/otto-recommender-system/test.jsonl'\n",
    "\n",
    "# print(\"Đang đọc và xử lý dữ liệu train...\")\n",
    "# sessions = []\n",
    "# products = []\n",
    "# interactions = []\n",
    "\n",
    "# # Đọc dữ liệu và chỉ sử dụng 100,000 sessions đầu tiên\n",
    "# max_sessions = 100000\n",
    "# session_count = 0\n",
    "\n",
    "# with open(train_file, 'r') as file:\n",
    "#     for line in file:\n",
    "#         if session_count >= max_sessions:\n",
    "#             break\n",
    "#         data = json.loads(line)\n",
    "#         session_id = data['session']\n",
    "#         session_count += 1\n",
    "#         for event in data['events']:\n",
    "#             aid = event['aid']\n",
    "#             event_type = event['type']\n",
    "#             weight = 1\n",
    "#             if event_type == 'carts':\n",
    "#                 weight = 3\n",
    "#             elif event_type == 'orders':\n",
    "#                 weight = 5\n",
    "#             sessions.append(session_id)\n",
    "#             products.append(aid)\n",
    "#             interactions.append(weight)\n",
    "\n",
    "# interaction_df = pd.DataFrame({'session': sessions, 'product': products, 'interaction': interactions})\n",
    "# print(f\"Đã xử lý {session_count} sessions.\")\n",
    "\n",
    "# # Lọc chỉ giữ lại top 2,000 sản phẩm phổ biến nhất\n",
    "# top_products = interaction_df['product'].value_counts().index[:2000]\n",
    "# interaction_df_filtered = interaction_df[interaction_df['product'].isin(top_products)]\n",
    "\n",
    "# # Tạo ma trận sparse từ dữ liệu đã giảm kích thước\n",
    "# interaction_matrix = interaction_df_filtered.pivot_table(index='session', columns='product', values='interaction', fill_value=0)\n",
    "# interaction_sparse = csr_matrix(interaction_matrix)\n",
    "\n",
    "# print(f\"Kích thước ma trận sau lọc: {interaction_sparse.shape}\")\n",
    "\n",
    "# # Bước 2: Áp dụng SVD\n",
    "# print(\"Đang áp dụng SVD...\")\n",
    "# svd = TruncatedSVD(n_components=10, random_state=42)  # Giảm số yếu tố ẩn\n",
    "# latent_matrix = svd.fit_transform(interaction_sparse)\n",
    "\n",
    "# # Tái tạo ma trận dự đoán\n",
    "# reconstructed_matrix = np.dot(latent_matrix, svd.components_)\n",
    "# reconstructed_df = pd.DataFrame(reconstructed_matrix, index=interaction_matrix.index, columns=interaction_matrix.columns)\n",
    "# print(\"Ma trận dự đoán đã được tạo.\")\n",
    "\n",
    "# # Bước 3: Dự đoán cho mỗi session trong tập test\n",
    "# print(\"Đang đọc và xử lý dữ liệu test...\")\n",
    "# predictions = []\n",
    "\n",
    "# with open(test_file, 'r') as file:\n",
    "#     for line in file:\n",
    "#         data = json.loads(line)\n",
    "#         session_id = data['session']\n",
    "#         if session_id in reconstructed_df.index:\n",
    "#             # Lấy top 20 sản phẩm dự đoán\n",
    "#             predicted_products = reconstructed_df.loc[session_id].sort_values(ascending=False).index[:20]\n",
    "#         else:\n",
    "#             predicted_products = interaction_df['product'].value_counts().index[:20]\n",
    "        \n",
    "#         # Tạo dự đoán cho các loại hành động\n",
    "#         predictions.append(f\"{session_id}_clicks,\" + \" \".join(map(str, predicted_products)))\n",
    "#         predictions.append(f\"{session_id}_carts,\" + \" \".join(map(str, predicted_products)))\n",
    "#         predictions.append(f\"{session_id}_orders,\" + \" \".join(map(str, predicted_products)))\n",
    "\n",
    "# # Bước 4: Ghi kết quả ra file submission.csv\n",
    "# output_file = '/kaggle/working/submission.csv'\n",
    "# print(f\"Đang ghi kết quả ra file {output_file}...\")\n",
    "\n",
    "# with open(output_file, 'w') as f:\n",
    "#     f.write(\"session_type,labels\\n\")\n",
    "#     for line in predictions:\n",
    "#         f.write(line + \"\\n\")\n",
    "\n",
    "# print(\"Dự đoán hoàn tất! File submission.csv đã được tạo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.kaggle.com/competitions/otto-recommender-system/discussion/382790"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Facebook](https://www.facebook.com/photo/?fbid=4257110251177146&set=pcb.1732429100877988)\n",
    "\n",
    "RAPIDS và PyTorch **có một số điểm tương đồng**, nhưng **chúng không giống nhau** và phục vụ các mục đích khác nhau. Hãy xem sự khác biệt và tương đồng giữa chúng:\n",
    "\n",
    "---\n",
    "\n",
    "### **Điểm giống nhau**:\n",
    "\n",
    "1. **Tăng tốc GPU**:\n",
    "    - Cả RAPIDS và PyTorch đều tận dụng sức mạnh của GPU thông qua nền tảng **CUDA** của NVIDIA để xử lý nhanh hơn.\n",
    "2. **Dùng trong lĩnh vực khoa học dữ liệu**:\n",
    "    - Cả hai đều là công cụ phổ biến trong cộng đồng khoa học dữ liệu và AI.\n",
    "3. **Mã nguồn mở**:\n",
    "    - Đều là mã nguồn mở, hỗ trợ cộng đồng lớn mạnh, có tài liệu phong phú và dễ mở rộng.\n",
    "4. **Ngôn ngữ Python**:\n",
    "    - Được thiết kế để sử dụng với Python, rất quen thuộc với các nhà khoa học dữ liệu và lập trình viên.\n",
    "\n",
    "---\n",
    "\n",
    "### **Điểm khác biệt**:\n",
    "\n",
    "|**Đặc điểm**|**RAPIDS**|**PyTorch**|\n",
    "|---|---|---|\n",
    "|**Mục đích chính**|Tăng tốc xử lý dữ liệu lớn và các tác vụ học máy truyền thống (machine learning).|Phát triển, huấn luyện và triển khai mô hình học sâu (deep learning).|\n",
    "|**Thư viện tương tự**|cuDF (giống pandas), cuML (giống scikit-learn), cuGraph (xử lý đồ thị)|Tương tự TensorFlow, nhưng linh hoạt hơn trong học sâu và mô hình hóa.|\n",
    "|**Xử lý dữ liệu**|Xử lý và phân tích dữ liệu lớn (Big Data Analytics).|Chỉ hỗ trợ xử lý dữ liệu như là một bước chuẩn bị trước khi huấn luyện mô hình học sâu.|\n",
    "|**Mô hình học sâu**|Không tập trung vào học sâu (deep learning), chỉ hỗ trợ học máy truyền thống (machine learning).|Tập trung hoàn toàn vào học sâu, bao gồm các mạng CNN, RNN, Transformers, GANs, v.v.|\n",
    "|**Khả năng tích hợp**|Tích hợp tốt với **Dask** (xử lý dữ liệu lớn), **Numba**, và các công cụ Big Data.|Tích hợp tốt với các công cụ như **TorchServe**, **ONNX**, và các framework AI khác.|\n",
    "|**Hiệu suất cao**|Hiệu quả khi xử lý khối lượng dữ liệu lớn trong thời gian thực.|Hiệu quả khi làm việc với mô hình phức tạp và cần tối ưu hóa toán học (backpropagation).|\n",
    "|**Đối tượng sử dụng**|Nhà khoa học dữ liệu cần tối ưu hóa xử lý dữ liệu và các thuật toán học máy.|Nhà nghiên cứu và kỹ sư AI làm việc với các mạng học sâu tiên tiến.|\n",
    "\n",
    "---\n",
    "\n",
    "### **Tóm lại**:\n",
    "\n",
    "- **RAPIDS** giống như một công cụ dành cho **khoa học dữ liệu và phân tích dữ liệu lớn**: nó hỗ trợ bạn làm việc nhanh hơn với các bước chuẩn bị và xử lý dữ liệu mà không cần viết mô hình học sâu phức tạp.\n",
    "- **PyTorch** là công cụ chuyên về **phát triển mô hình học sâu**, nơi bạn thiết kế các kiến trúc mạng phức tạp và tối ưu hóa thuật toán để đạt kết quả tốt nhất.\n",
    "\n",
    "Nếu bạn cần **kết hợp cả hai**:\n",
    "\n",
    "- Sử dụng RAPIDS để **xử lý dữ liệu nhanh trên GPU**, sau đó đưa dữ liệu vào PyTorch để huấn luyện mô hình học sâu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIẢI THÍCH ĐƠN GIẢN VÀ ỨNG DỤNG TRONG BÀI TOÁN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VỚI BỘ DATASET HIỆN TẠI KHÔNG CÓ THÔNG TIN VỀ SẢN PHẨM THÌ SAO DÙNG ???\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "Ví dụ:\n",
    "\n",
    "Bảng sở thích session:\n",
    "\n",
    "Session ID\tCông nghệ\tThời trang\n",
    "Session 1\t5\t1\n",
    "Session 2\t1\t4\n",
    "Session 3\t5\t1\n",
    "Bảng đặc điểm sản phẩm:\n",
    "\n",
    "Sản phẩm\tCông nghệ\tThời trang\n",
    "Sản phẩm A\t4\t2\n",
    "Sản phẩm B\t0\t5\n",
    "Sản phẩm C\t3\t1\n",
    "Sản phẩm D\t1\t4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cách Matrix Factorization hoạt động trong bài toán OTTO mà không cần thông tin chi tiết về sản phẩm**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Vấn đề: Không có thông tin sản phẩm chi tiết**\n",
    "\n",
    "Trong bài toán OTTO:\n",
    "- Dataset chỉ chứa **mã sản phẩm (article ID - `aid`)** và lịch sử hành vi của khách hàng (nhấp chuột, thêm vào giỏ, đặt mua).\n",
    "- Không có thông tin mô tả chi tiết sản phẩm (như \"sản phẩm thuộc nhóm công nghệ\" hay \"sản phẩm thời trang\").\n",
    "\n",
    "Vậy làm sao Matrix Factorization có thể hoạt động?\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Cách Matrix Factorization \"tự học\"**\n",
    "\n",
    "Matrix Factorization không cần biết trực tiếp thông tin chi tiết của sản phẩm vì:\n",
    "- Nó **tự tìm ra các đặc điểm ẩn (latent factors)** của sản phẩm và khách hàng từ dữ liệu tương tác.\n",
    "\n",
    "#### **Cụ thể:**\n",
    "1. **Ma trận tương tác (Interaction Matrix):**\n",
    "   - Mỗi khách hàng (hoặc session) tương ứng một hàng.\n",
    "   - Mỗi sản phẩm tương ứng một cột.\n",
    "   - Giá trị trong ma trận:\n",
    "     - Trọng số dựa trên hành động:  \n",
    "       `clicks = 1`, `carts = 3`, `orders = 5`.\n",
    "     - Nếu khách hàng chưa tương tác với sản phẩm: Ghi \"0\".\n",
    "\n",
    "   Ví dụ:\n",
    "\n",
    "   | Session ID | Product A | Product B | Product C | Product D |\n",
    "   |------------|-----------|-----------|-----------|-----------|\n",
    "   | Session 1  | 5         | 0         | 3         | 0         |\n",
    "   | Session 2  | 0         | 1         | 0         | 3         |\n",
    "   | Session 3  | 5         | 0         | 3         | 0         |\n",
    "\n",
    "2. **Phân tích ma trận (Matrix Factorization):**\n",
    "   - **Ma trận tương tác này có rất nhiều ô trống (0)**, do khách hàng không tương tác với mọi sản phẩm.\n",
    "   - Matrix Factorization (SVD) sẽ **phân tích ma trận** thành hai ma trận nhỏ hơn:\n",
    "     - **Ma trận sở thích khách hàng (Session-Latent Factors):** Đại diện cho sở thích tiềm ẩn của từng session.\n",
    "     - **Ma trận đặc điểm sản phẩm (Product-Latent Factors):** Đại diện cho các đặc điểm tiềm ẩn của từng sản phẩm.\n",
    "\n",
    "   Ví dụ, nếu có 2 yếu tố ẩn:\n",
    "   - **Ma trận sở thích khách hàng:**\n",
    "     | Session ID | Công nghệ | Thời trang |\n",
    "     |------------|-----------|------------|\n",
    "     | Session 1  | 5         | 1          |\n",
    "     | Session 2  | 1         | 4          |\n",
    "     | Session 3  | 5         | 1          |\n",
    "\n",
    "   - **Ma trận đặc điểm sản phẩm:**\n",
    "     | Product    | Công nghệ | Thời trang |\n",
    "     |------------|-----------|------------|\n",
    "     | Product A  | 4         | 2          |\n",
    "     | Product B  | 0         | 5          |\n",
    "     | Product C  | 3         | 1          |\n",
    "     | Product D  | 1         | 4          |\n",
    "\n",
    "3. **Tự động học các đặc điểm ẩn:**\n",
    "   - \"Công nghệ\" hay \"Thời trang\" trong bảng trên chỉ là ví dụ.\n",
    "   - Matrix Factorization không biết những yếu tố này là gì, nhưng **nó tự tìm ra** các đặc điểm ẩn bằng cách tối ưu hóa mối quan hệ giữa sessions và sản phẩm.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Dự đoán sản phẩm cho các session**\n",
    "\n",
    "1. **Tái tạo ma trận tương tác:**\n",
    "   - Nhân ma trận \"sở thích khách hàng\" với \"đặc điểm sản phẩm\" để tái tạo các giá trị trong ma trận gốc.\n",
    "   - Giá trị dự đoán cho mỗi ô là **mức độ phù hợp** giữa session và sản phẩm.\n",
    "\n",
    "   Ví dụ:\n",
    "   - Điểm dự đoán cho `Session 1` và `Product B`:\n",
    "     \\[\n",
    "     \\text{Score} = (5 \\times 0) + (1 \\times 5) = 5\n",
    "     \\]\n",
    "\n",
    "2. **Chọn sản phẩm phù hợp nhất:**\n",
    "   - Với mỗi session, chọn **top 20 sản phẩm có điểm dự đoán cao nhất** để gợi ý.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Vì sao không cần thông tin sản phẩm chi tiết?**\n",
    "\n",
    "Matrix Factorization vẫn hiệu quả vì:\n",
    "1. **Học các mẫu từ dữ liệu hành vi:**\n",
    "   - Dựa trên tương tác giữa sessions và sản phẩm, mô hình tự suy ra:\n",
    "     - Session nào có sở thích tương tự.\n",
    "     - Sản phẩm nào có đặc điểm giống nhau.\n",
    "\n",
    "   Ví dụ:\n",
    "   - Nếu Session 1 thích Product A và Session 2 cũng thích Product A, có khả năng cao cả hai session sẽ thích các sản phẩm tương tự như Product A.\n",
    "\n",
    "2. **Phát hiện đặc điểm tiềm ẩn:**\n",
    "   - Matrix Factorization không cần biết \"Công nghệ\" hay \"Thời trang\". Nó chỉ cần tìm ra các **yếu tố ẩn** giúp giải thích mối quan hệ giữa sessions và sản phẩm.\n",
    "\n",
    "3. **Dữ liệu tương tác đã đủ mạnh:**\n",
    "   - Dataset OTTO chứa nhiều thông tin về hành vi khách hàng (clicks, carts, orders). Đây là nguồn thông tin đủ để suy luận mối quan hệ giữa sessions và sản phẩm.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Tóm lại, Matrix Factorization trong OTTO:**\n",
    "- Không cần thông tin sản phẩm chi tiết, vì:\n",
    "  - SVD tự học các đặc điểm ẩn từ dữ liệu tương tác.\n",
    "- Mô hình sử dụng:\n",
    "  - **Lịch sử hành động của khách hàng** để hiểu sở thích.\n",
    "  - **Các sản phẩm tương tự** để đưa ra gợi ý hợp lý.\n",
    "- Kết quả:\n",
    "  - Dự đoán sản phẩm mà khách hàng có khả năng nhấp chuột, thêm vào giỏ hàng, hoặc đặt mua sau điểm bị cắt."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
